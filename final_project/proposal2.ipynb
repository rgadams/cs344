{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 344 -- Final Project Proposal 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to write Pink Floyd-esque songs using generative adversarial neural networks. I hope to learn about how adversarial neural networks work and use them in a way that can generate text. A potential use for this is to help struggling songwriters get ideas for lyrics. It will also be fun to see some of the potentially crazy things a machine can do with Pink Floyd's lyrics. They use a wide variety of vocabulary so the combinations it will come up with will be fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use generative adversarial networks because I know very little about them. Also, part of these networks has do do with generating things, which fits perfectly with a text generator.\n",
    "The primary resource I am using for generating text is found at https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/. This post uses the text of Alice in Wonderland to generate a paragraph in the same style. I have followed this tutorial to its end and was successfully able to generate similar text to that in the post. One thing that it has at the end is some potential improvements to the model. I am hoping to tackle one or two of those improvements, such as adding more layers and changing the LSTM layers to be \"stateful\" to maintain state across multiple batches. The post does not use adversarial networks, however, so I will be roughly following https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0 to create a discriminator and an adversarial model. This post creates a generative adversarial network that trains on the MNIST dataset, so I will have to adapt it for a text based generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generator itself looks something like this. Eventually I hope to add more layers and try tuning the dropout parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all I've been able to do is generate some text based on Alice in wonderland."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"  and there stood the queen\n",
    "in front of them, with her arms folded, frowning like a thunderstorm.\n",
    "\n",
    "‘a \"\n",
    "od you makegt toin at all ’our majesty,’ said the konk  ast the hodk turtle\n",
    "soen i shsu de lodd aueallen to meee the hoose of the toees. and the woide to tuirt wouhd the pooe tf thitg thth the roeer of the gorse sas oo the saake,\n",
    "aud the katter sar all care then  she house oo the koog of the tabbit, and she whrt oo tat in anyihusty to the toeer,\n",
    "and the thrt oo tae io the same tith her hoede on the rase sating ano the rase satien on the sas oo the rabbe at the caded in a lergle of the sabd to the was to the karee hn tee had sooe of the soees th the corrtuse ant ou the tas so the tabte,\n",
    "\n",
    "‘he iodr whrh the sey,’ said the caterpillar.\n",
    "\n",
    "‘he conster thene it ’asl’,’ said the katter. ‘i toon the woile the moos tf thing ’ou’ maae.’\n",
    "\n",
    "‘io wou kane th the dorso ’it,’ said the konk, and the hotphon reneiked\n",
    "toe pooele to herself, ‘io wou know then to be i fene the horse io the moose of the woide.’\n",
    "\n",
    "‘he iodst oo the semt wian ’ said the manch hare.\n",
    "\n",
    "‘ie cnn tfe seat to mea’ any tour it ’ said th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite bad, but this was using a version of the model with only one LTSM layer. I'm convinced that adding additional layers will improve the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Neural Networks are very cool, but can be somewhat scary. They can generate images of very real-looking people. This means that we will have a harder and harder time determining what is truly real and what is just generated by a network with the biases of its creators embedded into the model. We perhaps should have some kind of stamp that appears on generated data that tells us that it was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
