{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs and Text Generation\n",
    "Roy Adams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to use an LSTM to generate lyrics based on Pink Floyd songs. Apart from the learning aspects of this project, I hope to create a system that could help songwriters use word combinations that they had not previously considered. Pink Floyd's lyrics have always been a little strange, so seeing what a neural network can do with them would be interesting.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation With LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use a recurrent neural network (RNN). One of the most common RNNs is the LSTM (Long-Short Term Memory). In contrast to Feed-Forward networks, RNNs use previous results as inputs in addition to examples. This allows RNNs to keep track of important features that have already come through the network. In addition, RNNs use a different form of backpropogation than Feed-Forward networks. This is called backpropogation through time, which adds a time element to each part of the backpropogation process.  \n",
    "The biggest issue with RNNs was the vanishing and exploding gradient problem. With the added time element relate to the layers through multiplication, it is possible for the gradient descent of these networks to disappear with weights that are too small or get infinitely big with weights are too big.  \n",
    "LSTMs fix this problem. LSTMs contain a gated cell that exists outside of the natural flow of the neural network. These cells control what enters them by determining what pieces of information are important to remember (Skymind)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this project is heavily based on the blog/tutorial from https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/. This tutorial uses Alice in Wonderland text as their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the necessary libraries, we need to load and format the data. The dataset I used comes from https://www.kaggle.com/mousehead/songlyrics, which contains 57650 songs. I format the data by removing newline characters and ellipsis. I then combine all the Pink Floyd lyrics into one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"data\\\\songdata.csv\"\n",
    "df_songs = pd.read_csv(filename)\n",
    "df_pinkfloyd = df_songs.loc[df_songs['artist'] == 'Pink Floyd']\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.lower())\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.replace(r\"\\\\n\", \"\"))\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.replace(r\"\\.\\.\\.\", \"\"))\n",
    "raw_text = ' '.join(map(str, df_pinkfloyd['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the raw text of the lyrics, we need to format them in an intuitive way for neural networks. A common way to do this is to create enumerations of characters to integers. We also need to remember how we did this enumeration so that our network's output is able to be converted back to characters. After this enumeration, we define how many characters we want to keep track of at a time. We use this length when creating sequences of enumerated characters. We take seq_length characters as the the training x and select the next character chosen as the training y. We then normalize the x's to be a better input for the neural network. Finally, we change the y from being an integer to a one-hot encoded array of possible next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  96814\n",
      "Total Vocab:  49\n",
      "Total Patterns:  96714\n",
      "(96714, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# # summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# # prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create the LSTM model. It will have input shape (seq_length, 1). The first variation I tried was setting return_sequences=True for each LSTM layer. This gives us access to the output of each hidden state of the LSTM (“Understand the Difference...\", Machine Learning Mastery). I added a dropout layer after each LSTM layer to attempt to avoid overfitting. Finally, I flatten the 2D shape into a Dense output layer that matches the size of our possible next characters. This model took about 2.5 hours to train so I saved the trained weights in a file that can be loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# load the network weights\n",
    "filename = \"normal/weights-improvement-20-1.5803.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are ready to start generating text. We choose a random starting point in our data and use our network to predict what character will come next. From there we shift our pattern to include the new character and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have she pili the soy the gane wo far  \n",
      "the hand of see shacn in the siaee  \n",
      "and whll coe she cerhir the shpe  \n",
      "and when e fally aar gloe ard iung and ary?  \n",
      "and i wound in the fird  \n",
      "the dli is woule so the fere  \n",
      "the wall the wuud ou thels  \n",
      "she lighe wiet ir to pr your whet wou weet  \n",
      "and i can tie wale and dlra  \n",
      "and what eoedr and she saans  \n",
      "bnd the gey sase the carl  \n",
      "wht wat inow b shll  \n",
      "ald the wou way wha aallw of aod wha  \n",
      "gi the ware wou rhe kenw of the lrsh  \n",
      "i'me gond if io she way yhay you'  \n",
      "wou'll tee iack i gan you wwif wou ol the raie,  \n",
      "  \n",
      "i'v gring thog a bioother or wou the lirot  \n",
      "dre is foat  \n",
      "anw a shil wou'l hea mi the dorunng.  \n",
      "  \n",
      "and io the skan the soo'  \n",
      "you well wou way for the hier \n",
      "\n",
      "  \n",
      "the dar wou wase dood if the doe,  \n",
      "  \n",
      "she lena and your eaar bte hn the fie.  \n",
      "the way she doue of she dronnng the seie  \n",
      "and d'can wour oe the say ir fare  \n",
      "oubn aar feles aay  \n",
      "\n",
      " \n",
      "fet teet pi he hene the gomeer  \n",
      "and g can'tee heeh and wolls  \n",
      "she flgnw car fer s\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random starting point\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this is nonsense. There are a few recognizable words like \"the\", \"she\", \"and\", \"if\", and \"you\", but the majority of words are not English. One thing that is interesting is that the structure of a song is recognizable. Each line is fairly short and looks like it could belong to a song.  \n",
    "Disappointed with these results, I tried something slightly different. I turned the LSTMs into stateful mode. This meant adding the flag stateful=True and meant the previous training batch would be kept and influenced the current training batch. The biggest issue with implementing this was that the batch size had to be set to one, which took ten hours to train five epochs. The results from this test were even worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sur  \n",
      "  \n",
      "and the shoe the seen the sean  \n",
      "in the sare she see the shne  \n",
      "and i hane the wie the the suand  \n",
      "and i hene the siat  \n",
      "and i woue bere  \n",
      "and i she ne the dirt  \n",
      "i was hi the een  \n",
      "the lend ie the sioe the sane  \n",
      "and i would the she the fan  \n",
      "and i the le the she ie  \n",
      "the soane the wase  \n",
      "  \n",
      "the llar in the sian  \n",
      "and the siane and she way an i was she doar  \n",
      "and i wou doelt the sane the los  \n",
      "  \n",
      "and i wou doer the wanl the nan  \n",
      "the sane  \n",
      "the soane  \n",
      "wou and the riare the sare  \n",
      "\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, batch_input_shape=(1, X.shape[1], X.shape[2]), return_sequences=True, stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True, stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True, stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# load the network weights\n",
    "filename = \"stateful\\\\weights-improvement-stateful-05-2.1414.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random starting point\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\n\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. This definitely looks worse than the previous try. The sentences are shorter on average and start with \"the\" or \"and\" much too often.  \n",
    "Thinking the problem might be with generating individual characters at a time, I thought it might be interesting to try using full words instead. This required refactoring my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab:  17579\n",
      "Total Patterns:  17573\n",
      "(17573, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"data\\\\songdata.csv\"\n",
    "df_songs = pd.read_csv(filename)\n",
    "df_pinkfloyd = df_songs.loc[df_songs['artist'] == 'Pink Floyd']\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.lower())\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.replace(r\"\\n\", \"\"))\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.replace(r\"\\.\\.\\.\", \"\"))\n",
    "raw_text = ' '.join(map(str, df_pinkfloyd['text']))\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "unique_words = list(set(raw_text.split()))\n",
    "words = list(raw_text.split())\n",
    "\n",
    "word_to_int = dict((c, i) for i, c in enumerate(unique_words))\n",
    "int_to_word = dict((i, c) for i, c in enumerate(unique_words))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_words = len(words)\n",
    "print(\"Total Vocab: \", n_words)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 6\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_words - seq_length, 1):\n",
    "\tseq_in = words[i:i + seq_length]\n",
    "\tseq_out = words[i + seq_length]\n",
    "\tdataX.append([word_to_int[word] for word in seq_in])\n",
    "\t# print(\"x: \", [word_to_int[word] for word in seq_in])\n",
    "\tdataY.append(word_to_int[seq_out])\n",
    "\t# print(\"y: \", word_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_words)\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32, return_sequences=True, activation='tanh'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# load the network weights\n",
    "filename = \"words\\\\weights-improvement-04-6.6583.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent sent \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random starting point\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_words)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\n\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not working either. I believe the issue here is that this model is not sampling properly from the distribution of possibilities, but rather grabbing the most likely word. This could be escalated if during the training some weights started to become insignificant, making the word \"sent\" a much higher probability than any other word, regardless of context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chollet's approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the blog's approach, Francois Chollet, author of Deep Learning with Python, samples from a distribution of possible characters rather than using the most likely. Much of the text below was taken directly from the accompanying IPython Notebook for chapter 8.1 (https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "\n",
    "filename = \"data\\\\songdata.csv\"\n",
    "df_songs = pd.read_csv(filename)\n",
    "df_pinkfloyd = df_songs.loc[df_songs['artist'] == 'Pink Floyd']\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.lower())\n",
    "df_pinkfloyd = df_pinkfloyd.apply(lambda x: x.astype(str).str.replace(r\"\\.\\.\\.\", \"\"))\n",
    "text = ' '.join(map(str, df_pinkfloyd['text']))\n",
    "print('Corpus length:', len(text))\n",
    "\n",
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    print('epoch', epoch)\n",
    "    filepath=\"chollet/weights-improvement-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \"oard for the american tour,  \n",
      "and maybe you'll make it to th\"\n",
      "------ temperature: 0.2\n",
      "oard for the american tour,  \n",
      "and maybe you'll make it to the starter to me  \n",
      "you can you stand the wart old would the sunshing on the sunshing the sunshing was so stream of the sand.  \n",
      "and i rise in the sunshing wood  \n",
      "and you sit would be so look how.  \n",
      "  \n",
      "in his on the sand of the morning  \n",
      "  \n",
      "like the sange in his break in her  \n",
      "you can you then we there was it  \n",
      "  \n",
      "ooooh, i can you then in the sunshing so calling in the sunshing on the sunshing was so\n",
      "------ temperature: 0.5\n",
      "e sunshing so calling in the sunshing on the sunshing was so gig hold of a way  \n",
      "should stand the words they cai to try  \n",
      "something it fly live  \n",
      "a flight i start deep all arout?  \n",
      "  \n",
      "walk to ching you feel  \n",
      "do you helver  \n",
      "you are we going for the watering in your sween  \n",
      "when we gold watching all of the sand.  \n",
      "should sound you sit for me.  \n",
      "  \n",
      "ooooh, wings and so you back.  \n",
      "hould be so nice  \n",
      "it would be so nice  \n",
      "to make the shot athing in the sand. \n",
      "------ temperature: 1.0\n",
      "\n",
      "it would be so nice  \n",
      "to make the shot athing in the sand.  \n",
      "  \n",
      "ooooh, everywhat in you get so you holf offelly spthing with glaigns in serp dit the phowe  \n",
      "you areside, he's coming on you break lear  \n",
      "  \n",
      "i poon a river  \n",
      "think in their gletimes it lieting and just and the bauze and so in the had see  \n",
      "gather there wast to shoulded his bif in and spee in a priem  \n",
      "  \n",
      "waiting sound in thes sade  \n",
      "tleath you loke ferels you, should in knowor, on the treated\n",
      "------ temperature: 1.2\n",
      "tleath you loke ferels you, should in knowor, on the treated tain.  \n",
      "if i mome love in titer  \n",
      "ecally up and tig me me.  \n",
      "another animalion man in the pack and water ti, to meta.  \n",
      "dramkles?  \n",
      "are aslung up, aory rafesling for gramp in  \n",
      "  \n",
      "his gind offer the broe.  \n",
      "in s'stiel mirds as succhefichwall was are  \n",
      "  \n",
      "scavery, just the nightr fat to meer  \n",
      "and cold out but so night ain  \n",
      "i hand you  \n",
      "in the misters trecting way.  \n",
      "with, an of michne stanes  \n",
      "s\n"
     ]
    }
   ],
   "source": [
    "filename = \"chollet\\\\weights-improvement-0.9230.hdf5\"\n",
    "\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Select a text seed at random\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(generated_text)\n",
    "\n",
    "    # We generate 400 characters\n",
    "    for i in range(400):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting somewhere! The different temperatures that we see are changing how the sampler pulls from the character distributions. The higher the temperature, the more easily the sampler selects less likely characters as the next character, whereas the low temperatures make it harder for the sampler to pick the unlikely values. This is why the lower temperature sections seem like more standard English than their high temperature counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the model from the blogpost and Challot's model, it is clear that Challot's is better. He organizes the data in a very similar way to the blog and both the networks are LSTMs, but I believe the clear advantage Challot has is due to his use of sampling. We see real English words arranged in a way that does not make complete sense, but is readable and not too repetetive.  \n",
    "The one thing that I like about both Challot and the blog's model is that they output lines in a format that could be interpreted as song lyrics, even though they are nonsensical. There are very few words per line, which makes it look a little like poetry.  \n",
    "One reason I believe the blog's version did not work is because of the limited dataset. For both character models and the word model I simply did not have enough Pink Floyd songs to train the model effectively. One idea to fix this is to expand the dataset to include all psychadelic British rock groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the LSTMs generated nonsense for me, it is reasonable to assume that more data would have improved its capabilities. With this assumption, I could imagine someone using an LSTM to create a chatbot. This chatbot could be trained on millions of conversations between real people and learn what kind of responses are natural. This could become an ethical issue if the chatbot pretends to be a human. It could try to do nefarious things like prompt for personal information, bully people online, or spread false information.  \n",
    "We have already seen a rise in fake news on social media, and I wouldn't be surprised if internet trolls started turning to text generation systems to spew out fake news articles. If an article doesn't get enough clicks, the developer could simply tweak something in the training to make it more realistic or outrageous, depending on the situation.\n",
    "We will have to be alert to the possibility that we are communicating with an AI rather than a real human being. With further developments in text generation and larger datasets this is becomming a real possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fedus, William, et al. “MaskGAN: Better Text Generation via Filling in the______.”  ICLR 2018, arxiv.org/abs/1801.07736.  \n",
    "“A Beginner's Guide to LSTMs and Recurrent Neural Networks.” Skymind, skymind.ai/wiki/lstm.  \n",
    "“Understand the Difference Between Return Sequences and Return States for LSTMs in Keras.” Machine Learning Mastery, 16 Oct. 2017, machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
